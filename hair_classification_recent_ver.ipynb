{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hair_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMA9te8RlPgBG4k51KTnW6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/2020_Image_Lab/blob/master/hair_classification_recent_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Me4zvhOxFsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d403ac8-cfda-46af-c36a-b4a213e2e970"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E3B51tOxpdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-io\n",
        "!pip install -q pydicom"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKqZAwEHxsb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "from pydicom.data import get_testdata_files\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtoGbJLxx9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pydicom"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tHcXJKMykrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the random seed number for reproducible results\n",
        "seedNum = 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg09uqnXJnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the TensorFlow version to 2.x in Colab\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t0saa6fJqCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(seedNum)\n",
        "import numpy as np\n",
        "np.random.seed(seedNum)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seedNum)\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import smtplib\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.image import imread\n",
        "from datetime import datetime\n",
        "from email.message import EmailMessage"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZHrr-DyJsET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "50719d14-b456-473e-985b-1dc8d37dac24"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, \n",
            "and then re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKkb9iXSJuN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\tfig, axs = pyplot.subplots(2, 1, figsize=(12,12))\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='red', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "\tpyplot.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYInvzMSJwcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24fb6335-65b2-42a9-e4b7-5bafdd76a65a"
      },
      "source": [
        "startTimeScript = datetime.now()\n",
        "\n",
        "# Set up the number of CPU cores available for multi-thread processing\n",
        "n_jobs = -1\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
        "notifyStatus = False\n",
        "\n",
        "# Set up the mountStorage flag to mount G Drive for storing files (setting True will mount the drive!)\n",
        "mountStorage = False"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwo8RcnbO0IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the default optimizer for the remaining portion of the script\n",
        "default_opt = Adam(learning_rate=0.001)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUjLw4i9O6-Z",
        "colab_type": "text"
      },
      "source": [
        "# **Section 1. Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTnrjedEVClW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define location of dataset\n",
        "folder = '/content/gdrive/My Drive/ISIC-melanoma/train/'\n",
        "folder2 = '/content/gdrive/My Drive/ISIC-melanoma/hair_labeling/'\n",
        "# plot first few images\n",
        "hair_label = []\n",
        "image_field = []\n",
        "\n",
        "# start_num = 15719\n",
        "# end_num = 100000\n",
        "\n",
        "# start_num = 100000\n",
        "# end_num = 405555\n",
        "\n",
        "start_num = 405652\n",
        "end_num = 695029\n",
        "\n",
        "# start_num = 695059\n",
        "# end_num = 999642\n",
        "\n",
        "# start_num = 1000328\n",
        "# end_num = 1307396\n",
        "\n",
        "# start_num = 1307419\n",
        "# end_num = 1601764\n",
        "\n",
        "# start_num = 1601784\n",
        "# end_num = 1894741\n",
        "\n",
        "# start_num = 1894782\n",
        "# end_num = 2188465\n",
        "\n",
        "# start_num = 2188566\n",
        "# end_num = 2491594\n",
        "\n",
        "file_name = str(start_num) + '_' + str(end_num) + '.csv'\n",
        "\n",
        "target_row = 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJaAV61IVFAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "9eb57625-6d6b-4844-e476-6e32ace8b24d"
      },
      "source": [
        "######hair이 있는 사진들에 대해서!###########\n",
        "import csv\n",
        "\n",
        "hair_image_file = []\n",
        "\n",
        "ct = 1\n",
        "\n",
        "with open(folder2 + file_name, newline='') as myfile:\n",
        "    reader  = csv.reader(myfile, delimiter=',')\n",
        "    for i in reader:\n",
        "        if i[1] == '1':\n",
        "          hair_image_file.append(i[0])\n",
        "        ct += 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b040b1871821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mreader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/ISIC-melanoma/hair_labeling/405652_695029.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybQNecoIXCPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "for i in range (1, 10):#원랜 (1, ct + 1)\n",
        "  pyplot.subplot(330 + i)\n",
        "  if os.path.isfile(folder + hair_image_file[i] + '.dcm'):\n",
        "    filename = folder + hair_image_file[i] + '.dcm'\n",
        "    dataset = pydicom.dcmread(filename)\n",
        "\n",
        "    image_bytes = tf.io.read_file(filename)\n",
        "\n",
        "    image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)\n",
        "\n",
        "    skipped = tfio.image.decode_dicom_image(image_bytes, on_error='skip', dtype=tf.uint8)\n",
        "\n",
        "    image_info = np.squeeze(image.numpy())\n",
        "\n",
        "    rows = int(dataset.Rows) #높이\n",
        "    cols = int(dataset.Columns) #너비\n",
        "\n",
        "    cropped = image_info[int(rows/2 - cols/3) :int(rows/2 + cols/3), 0:cols]\n",
        "    if cols/rows > 1.5:\n",
        "      cropped = image_info[0 : rows, int(cols/2 - rows*3/4):int(cols/2 + rows*3/4)]\n",
        "    \n",
        "    cropped = cv2.resize(cropped, dsize=(600, 400), interpolation=cv2.INTER_CUBIC)             #resize 진행\n",
        "    \n",
        "  plt.imshow(cropped)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG62I3wcG97B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hair_image_file = []\n",
        "\n",
        "ct = 1\n",
        "\n",
        "with open(folder2 + file_name, newline='') as myfile:\n",
        "    reader  = csv.reader(myfile, delimiter=',')\n",
        "    for i in reader:\n",
        "        if i[1] == '0':\n",
        "          hair_image_file.append(i[0])\n",
        "        ct += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXRHMm65xzAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "##center crop size ratio\n",
        "width_ratio = 1\n",
        "height_ratio = 2/3\n",
        "\n",
        "\n",
        "for i in range (1, 10):#원랜 (1, ct + 1)\n",
        "  pyplot.subplot(330 + i)\n",
        "  if os.path.isfile(folder + hair_image_file[i] + '.dcm'):\n",
        "    filename = folder + hair_image_file[i] + '.dcm'\n",
        "    dataset = pydicom.dcmread(filename)\n",
        "\n",
        "    image_bytes = tf.io.read_file(filename)\n",
        "\n",
        "    image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)\n",
        "\n",
        "    skipped = tfio.image.decode_dicom_image(image_bytes, on_error='skip', dtype=tf.uint8)\n",
        "\n",
        "    image_info = np.squeeze(image.numpy())\n",
        "\n",
        "    rows = int(dataset.Rows) #높이\n",
        "    cols = int(dataset.Columns) #너비\n",
        "\n",
        "    # left = 0 # 여기 작성하다 맘\n",
        "    # top = int((rows - (cols*height_ratio)) / 2)\n",
        "    # right = 0\n",
        "    # bottom = int((rows + (cols*height_ratio)) / 2)\n",
        "\n",
        "    # image = image_info.crop((left, top, right, bottom))   #center crop 진행\n",
        "    cropped = image_info[int(rows/2 - cols/3) :int(rows/2 + cols/3), 0:cols]\n",
        "    if cols/rows > 1.5:\n",
        "      cropped = image_info[0 : rows, int(cols/2 - rows*3/4):int(cols/2 + rows*3/4)]\n",
        "    # print(cropped)\n",
        "    cropped = cv2.resize(cropped, dsize=(600, 400), interpolation=cv2.INTER_CUBIC)             #resize 진행\n",
        "    \n",
        "    # print(image_info.shape)\n",
        "    # print(cropped.shape)\n",
        "  plt.imshow(cropped)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piSrCna8SvDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# organize dataset into a useful structure\n",
        "from os import makedirs\n",
        "from os import listdir\n",
        "from shutil import copyfile\n",
        "from random import seed\n",
        "from random import random\n",
        "import scipy.misc\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# create directories\n",
        "dataset_home = '/content/gdrive/My Drive/dataset_nonhair_vs_hair/'\n",
        "subdirs = ['train/', 'test/']\n",
        "for subdir in subdirs:\n",
        "\t# create label subdirectories\n",
        "\tlabeldirs = ['nonhair/', 'hair/']\t\t#dog == nonhair, cat == hair\n",
        "\tfor labldir in labeldirs:\n",
        "\t\tnewdir = dataset_home + subdir + labldir\n",
        "\t\tmakedirs(newdir, exist_ok=True)\n",
        "# seed random number generator\n",
        "seed(seedNum)\n",
        "# define ratio of pictures to use for validation\n",
        "val_ratio = 0.25\n",
        "# copy training dataset images into subdirectories\n",
        "# src_directory = 'train/'\n",
        "src_directory = '/content/gdrive/My Drive/ISIC-melanoma/train/'\n",
        "\n",
        "for file in listdir(src_directory):\n",
        "\tsrc = src_directory + '/' + file\n",
        "\tdst_dir = 'train/'\n",
        "\tif random() < val_ratio:\n",
        "\t\tdst_dir = 'test/'\n",
        "\t# if file.startswith('hair'):\t\t\t\t\t\t##요기 if문을 label 0, 1 배열 안에 있는 파일 숫자인지 확인하는걸 만들어서 바꾸면 될 듯. csv에서 해당 파일 레이블이 1인가. 아래코드로!\n",
        "\twith open(folder2 + file_name, newline='') as myfile:\n",
        "\t\treader  = csv.reader(myfile, delimiter=',')\n",
        "\t\tct = 1\n",
        "\t\tfor i in reader:\n",
        "\t\t\tif i[1] == '1':\n",
        "\t\t\t\tfile = i[0]\n",
        "\t\t\t\tdst = dataset_home + dst_dir + 'hair/'  + file + '.jpg'\n",
        "\t\t\t\t# copyfile(src, dst)\n",
        "\t\t\t\tdataset = pydicom.dcmread(filename)\n",
        "\n",
        "\t\t\t\timage_bytes = tf.io.read_file(filename)\n",
        "\n",
        "\t\t\t\timage = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)\n",
        "\n",
        "\t\t\t\tskipped = tfio.image.decode_dicom_image(image_bytes, on_error='skip', dtype=tf.uint8)\n",
        "\n",
        "\t\t\t\timage_info = np.squeeze(image.numpy())\n",
        "\n",
        "\t\t\t\trows = int(dataset.Rows) #높이\n",
        "\t\t\t\tcols = int(dataset.Columns) #너비\n",
        "\n",
        "\t\t\t\tcropped = image_info[int(rows/2 - cols/3) :int(rows/2 + cols/3), 0:cols]\n",
        "\t\t\t\tif cols/rows > 1.5:\n",
        "\t\t\t\t\tcropped = image_info[0 : rows, int(cols/2 - rows*3/4):int(cols/2 + rows*3/4)]\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcropped = cv2.resize(cropped, dsize=(600, 400), interpolation=cv2.INTER_CUBIC)\n",
        "\t\t\n",
        "\t\t\t\t# scipy.misc.toimage(cropped, cmin=0.0, cmax=...).save(dst)\n",
        "\t\t\t\t# imageio.imwrite(dst, cropped[:, :, 0])\n",
        "\t\t\t\tim = Image.fromarray(cropped)\n",
        "\t\t\t\tim.save(dst)\n",
        "\t\t\telif i[1] == '0':\n",
        "\t\t\t\tfile = i[0]\n",
        "\t\t\t\tdst = dataset_home + dst_dir + 'hair/'  + file + '.jpg'\n",
        "\t\t\t\t# copyfile(src, dst)\n",
        "\t\t\t\tdataset = pydicom.dcmread(filename)\n",
        "\n",
        "\t\t\t\timage_bytes = tf.io.read_file(filename)\n",
        "\n",
        "\t\t\t\timage = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)\n",
        "\n",
        "\t\t\t\tskipped = tfio.image.decode_dicom_image(image_bytes, on_error='skip', dtype=tf.uint8)\n",
        "\n",
        "\t\t\t\timage_info = np.squeeze(image.numpy())\n",
        "\n",
        "\t\t\t\trows = int(dataset.Rows) #높이\n",
        "\t\t\t\tcols = int(dataset.Columns) #너비\n",
        "\n",
        "\t\t\t\tcropped = image_info[int(rows/2 - cols/3) :int(rows/2 + cols/3), 0:cols]\n",
        "\t\t\t\tif cols/rows > 1.5:\n",
        "\t\t\t\t\tcropped = image_info[0 : rows, int(cols/2 - rows*3/4):int(cols/2 + rows*3/4)]\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcropped = cv2.resize(cropped, dsize=(600, 400), interpolation=cv2.INTER_CUBIC)\n",
        "\t\t\n",
        "\t\t\t\t# scipy.misc.toimage(cropped, cmin=0.0, cmax=...).save(dst)\n",
        "\t\t\t\t# imageio.imwrite(dst, cropped[:, :, 0])\n",
        "\t\t\t\tim = Image.fromarray(cropped)\n",
        "\t\t\t\tim.save(dst)\n",
        "\t\t\n",
        "\t\t\tprint(ct)\n",
        "\t\t\tct = ct + 1\n",
        "  #         hair_image_file.append(i[0])\n",
        "\t\t# dst = dataset_home + dst_dir + 'hair/'  + file\n",
        "\t\t# copyfile(src, dst)\n",
        "\t# elif file.startswith('nonhair'):\n",
        "\t# \tdst = dataset_home + dst_dir + 'nonhair/'  + file\n",
        "\t# \tcopyfile(src, dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB4EheXzrHH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pydicom.dcmread(filename)\n",
        "\n",
        "image_bytes = tf.io.read_file(filename)\n",
        "\n",
        "image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)\n",
        "\n",
        "skipped = tfio.image.decode_dicom_image(image_bytes, on_error='skip', dtype=tf.uint8)\n",
        "\n",
        "image_info = np.squeeze(image.numpy())\n",
        "\n",
        "rows = int(dataset.Rows) #높이\n",
        "cols = int(dataset.Columns) #너비\n",
        "\n",
        "cropped = image_info[int(rows/2 - cols/3) :int(rows/2 + cols/3), 0:cols]\n",
        "if cols/rows > 1.5:\n",
        "  cropped = image_info[0 : rows, int(cols/2 - rows*3/4):int(cols/2 + rows*3/4)]\n",
        "  \n",
        "cropped = cv2.resize(cropped, dsize=(600, 400), interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTLBFg1oSpdm",
        "colab_type": "text"
      },
      "source": [
        "# **Section 2. Fit and Evaluate Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T13x8rU1SoIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# baseline model with One Block VGG Model\n",
        "startTimeModule = datetime.now()\n",
        "tf.random.set_seed(seedNum)\n",
        "\n",
        "# define cnn model\n",
        "def define_model1():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(400, 600, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer=default_opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel1 = define_model1()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = datagen.flow_from_directory('/content/gdrive/My Drive/dataset_nonhair_vs_hair/train/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(400, 600))\n",
        "\ttest_it = datagen.flow_from_directory('/content/gdrive/My Drive/dataset_nonhair_vs_hair/test/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(400, 600))\n",
        "\t# fit model\n",
        "\thistory = model1.fit(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model1.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()\n",
        "print('Total time for the model processing:', (datetime.now() - startTimeModule))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YMcqNzAS_L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# baseline model with Two Block VGG Model\n",
        "startTimeModule = datetime.now()\n",
        "tf.random.set_seed(seedNum)\n",
        "\n",
        "# define cnn model\n",
        "def define_model2():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer=default_opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel2 = define_model2()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\ttest_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\t# fit model\n",
        "\thistory = model2.fit(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model2.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()\n",
        "print('Total time for the model processing:', (datetime.now() - startTimeModule))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0OORFWSTBwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# baseline model with Three Block VGG Model\n",
        "startTimeModule = datetime.now()\n",
        "tf.random.set_seed(seedNum)\n",
        "\n",
        "# define cnn model\n",
        "def define_model3():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer=default_opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel3 = define_model3()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\ttest_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\t# fit model\n",
        "\thistory = model3.fit(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model3.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()\n",
        "print('Total time for the model processing:', (datetime.now() - startTimeModule))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67jD0r7STHlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# baseline model with VGG-3 and dropout (variation #1)\n",
        "startTimeModule = datetime.now()\n",
        "tf.random.set_seed(seedNum)\n",
        "\n",
        "# define cnn model\n",
        "def define_model4():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer=default_opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel4 = define_model4()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterator\n",
        "\ttrain_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\ttest_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\t# fit model\n",
        "\thistory = model4.fit(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model4.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()\n",
        "print('Total time for the model processing:', (datetime.now() - startTimeModule))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}